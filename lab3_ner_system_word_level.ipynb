{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll try to create NER system.\n",
    "The purpose of this system is to extract Persons, Organizations and Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://researchkb.files.wordpress.com/2014/02/ner.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, codecs, csv\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "# visualization\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of keras layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense, Dropout\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset can be downloaded from this source:\n",
    "https://github.com/EuropeanaNewspapers/ner-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enp_FR.bnf.bio.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emmanuel I-PER\n",
      "DESOLES I-PER\n",
      "de O\n",
      "LOU O\n",
      "Directeur O\n",
      "politique O\n",
      "BГЉ>ГЂCTION O\n",
      "ET O\n",
      "ADMINISTRATION O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "types = []\n",
    "for item in text.split('\\n'):\n",
    "    item = item.strip()\n",
    "    if len(item) == 0:\n",
    "        continue\n",
    "    [w, t] = item.split(' ')\n",
    "    words.append(w)\n",
    "    types.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC': 0, 'O': 1, 'I-PER': 2, 'I-ORG': 3}\n"
     ]
    }
   ],
   "source": [
    "unique_types = list(set(types))\n",
    "type2id = {x:index for index, x in enumerate(unique_types)}\n",
    "print(type2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word2count = Counter(words)\n",
    "MAX_WORD_COUNT = 30000\n",
    "top_words = [x[0] for x in sorted(word2count.items(), key=lambda x: x[1], reverse=True)][:MAX_WORD_COUNT]\n",
    "word2id = {x:index+1 for index, x in enumerate(top_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = train_test_split(list(zip(words, types)), train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, None, 200)         6000200   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, None, 400)         641600    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, None, 4)           1604      \n",
      "_________________________________________________________________\n",
      "crf_10 (CRF)                 (None, None, 10)          170       \n",
      "=================================================================\n",
      "Total params: 6,643,574\n",
      "Trainable params: 6,643,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(10, sparse_target=True)\n",
    "\n",
    "input = Input(shape=(None,))\n",
    "out = Embedding(input_dim=len(word2id)+1, output_dim=200)(input)\n",
    "out=Dropout(0.01)(out)\n",
    "out = Bidirectional(LSTM(200, activation='relu', return_sequences=True))(out)\n",
    "out = Dense(len(type2id), activation='softmax')(out)\n",
    "out =crf(out)\n",
    "model = Model(input, out)\n",
    "model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordId(w):\n",
    "    return 0 if not w in word2id else word2id[w]\n",
    "\n",
    "def gen_batches(dataset, batch_size=64, seq_size=32, batch_count=100):\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    features = np.zeros((batch_size, seq_size))\n",
    "    labels = np.zeros((batch_size, seq_size, len(type2id)))\n",
    "    for _ in range(batch_count):\n",
    "        for seq_index in range(batch_size):\n",
    "            left = random.randint(0, len(dataset) - seq_size)\n",
    "            features[seq_index,:] = [getWordId(x[0]) for x in dataset[left:left+seq_size]]\n",
    "            labels[seq_index,:] = 0\n",
    "            for i,(_,t) in enumerate(dataset[left:left+seq_size]):\n",
    "                labels[seq_index,i] = 0\n",
    "                labels[seq_index,i,type2id[t]] = 1\n",
    "        yield features, labels\n",
    "        \n",
    "def encode_text(sentence):\n",
    "    words = sentence.split()\n",
    "    result = np.zeros((len(words),))\n",
    "    for i,w in enumerate(words):\n",
    "        result[i] = getWordId(w)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import os, shutil\n",
    "\n",
    "\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "logs_dir = './logs'\n",
    "callback = TensorBoard(logs_dir)\n",
    "callback.set_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-LOC', 'O', 'I-PER', 'I-ORG']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train 1.0137541\n",
      "test 0.90717036\n",
      "1\n",
      "train 0.86487234\n",
      "test 0.8513824\n",
      "2\n",
      "train 1.1388707\n",
      "test 1.2253613\n",
      "3\n",
      "train 1.187392\n",
      "test 1.147805\n",
      "4\n",
      "train 1.1118658\n",
      "test 1.074653\n",
      "5\n",
      "train 1.0406841\n",
      "test 1.0062783\n",
      "6\n",
      "train 0.9747646\n",
      "test 0.94327676\n",
      "7\n",
      "train 0.91443115\n",
      "test 0.88625985\n",
      "8\n",
      "train 0.8607714\n",
      "test 0.8348105\n",
      "9\n",
      "train 0.8132162\n",
      "test 0.7916608\n",
      "10\n",
      "train 0.7715155\n",
      "test 0.75343114\n",
      "11\n",
      "train 0.73651576\n",
      "test 0.7233517\n",
      "12\n",
      "train 0.70887506\n",
      "test 0.69655097\n",
      "13\n",
      "train 0.68594545\n",
      "test 0.676216\n",
      "14\n",
      "train 0.6670594\n",
      "test 0.6609539\n",
      "15\n",
      "train 0.65152615\n",
      "test 0.6457351\n",
      "16\n",
      "train 0.64005005\n",
      "test 0.63508034\n",
      "17\n",
      "train 0.62988985\n",
      "test 0.62694365\n",
      "18\n",
      "train 0.6215066\n",
      "test 0.6200566\n",
      "19\n",
      "train 0.61611867\n",
      "test 0.61394095\n",
      "20\n",
      "train 0.6115825\n",
      "test 0.61069304\n",
      "21\n",
      "train 0.6047672\n",
      "test 0.606082\n",
      "22\n",
      "train 0.60289836\n",
      "test 0.6019052\n",
      "23\n",
      "train 0.59874696\n",
      "test 0.5982297\n",
      "24\n",
      "train 0.5952654\n",
      "test 0.59464014\n",
      "25\n",
      "train 0.59361255\n",
      "test 0.59372914\n",
      "26\n",
      "train 0.59064424\n",
      "test 0.59266216\n",
      "27\n",
      "train 0.58902705\n",
      "test 0.5879674\n",
      "28\n",
      "train 0.5862354\n",
      "test 0.58812064\n",
      "29\n",
      "train 0.5851042\n",
      "test 0.58615816\n",
      "30\n",
      "train 0.5842675\n",
      "test 0.58534193\n",
      "31\n",
      "train 0.5840639\n",
      "test 0.58410186\n",
      "32\n",
      "train 0.5822325\n",
      "test 0.5831606\n",
      "33\n",
      "train 0.58024323\n",
      "test 0.5818543\n",
      "34\n",
      "train 0.581495\n",
      "test 0.57979727\n",
      "35\n",
      "train 0.5779431\n",
      "test 0.5804392\n",
      "36\n",
      "train 0.578429\n",
      "test 0.5790188\n",
      "37\n",
      "train 0.5777463\n",
      "test 0.5788194\n",
      "38\n",
      "train 0.5778574\n",
      "test 0.5786978\n",
      "39\n",
      "train 0.5775784\n",
      "test 0.5777906\n",
      "40\n",
      "train 0.5748258\n",
      "test 0.5773204\n",
      "41\n",
      "train 0.57646686\n",
      "test 0.5771277\n",
      "42\n",
      "train 0.5736121\n",
      "test 0.5757508\n",
      "43\n",
      "train 0.5748544\n",
      "test 0.575797\n",
      "44\n",
      "train 0.5723\n",
      "test 0.5753783\n",
      "45\n",
      "train 0.57417226\n",
      "test 0.57446563\n",
      "46\n",
      "train 0.57294494\n",
      "test 0.5744742\n",
      "47\n",
      "train 0.573656\n",
      "test 0.57464415\n",
      "48\n",
      "train 0.5730324\n",
      "test 0.5735668\n",
      "49\n",
      "train 0.57286745\n",
      "test 0.57397866\n",
      "50\n",
      "train 0.57261634\n",
      "test 0.57331455\n",
      "51\n",
      "train 0.5720656\n",
      "test 0.572606\n",
      "52\n",
      "train 0.5702239\n",
      "test 0.57228625\n",
      "53\n",
      "train 0.5703088\n",
      "test 0.5732362\n",
      "54\n",
      "train 0.5708214\n",
      "test 0.57229924\n",
      "55\n",
      "train 0.5708897\n",
      "test 0.57171774\n",
      "56\n",
      "train 0.57095134\n",
      "test 0.57089627\n",
      "57\n",
      "train 0.5708442\n",
      "test 0.5721836\n",
      "58\n",
      "train 0.5709435\n",
      "test 0.570522\n",
      "59\n",
      "train 0.5698219\n",
      "test 0.5704099\n",
      "60\n",
      "train 0.5685417\n",
      "test 0.5707423\n",
      "61\n",
      "train 0.5689767\n",
      "test 0.57055396\n",
      "62\n",
      "train 0.5691851\n",
      "test 0.56937605\n",
      "63\n",
      "train 0.5695474\n",
      "test 0.57120013\n",
      "64\n",
      "train 0.5679332\n",
      "test 0.56971115\n",
      "65\n",
      "train 0.56942046\n",
      "test 0.57071507\n",
      "66\n",
      "train 0.56646365\n",
      "test 0.56910586\n",
      "67\n",
      "train 0.56722224\n",
      "test 0.5695661\n",
      "68\n",
      "train 0.5695876\n",
      "test 0.56992686\n",
      "69\n",
      "train 0.5672569\n",
      "test 0.5708165\n",
      "70\n",
      "train 0.5664295\n",
      "test 0.56857663\n",
      "71\n",
      "train 0.5666926\n",
      "test 0.56740904\n",
      "72\n",
      "train 0.56734556\n",
      "test 0.5687125\n",
      "73\n",
      "train 0.56682503\n",
      "test 0.5689423\n",
      "74\n",
      "train 0.56670845\n",
      "test 0.569297\n",
      "75\n",
      "train 0.56690574\n",
      "test 0.56785387\n",
      "76\n",
      "train 0.566499\n",
      "test 0.5680336\n",
      "77\n",
      "train 0.5667664\n",
      "test 0.56848216\n",
      "78\n",
      "train 0.5672935\n",
      "test 0.56819415\n",
      "79\n",
      "train 0.5676702\n",
      "test 0.5667961\n",
      "80\n",
      "train 0.56734\n",
      "test 0.566851\n",
      "81\n",
      "train 0.5648409\n",
      "test 0.5668225\n",
      "82\n",
      "train 0.56440043\n",
      "test 0.5667904\n",
      "83\n",
      "train 0.5663272\n",
      "test 0.56667703\n",
      "84\n",
      "train 0.5657202\n",
      "test 0.5656358\n",
      "85\n",
      "train 0.56602645\n",
      "test 0.5685155\n",
      "86\n",
      "train 0.5646306\n",
      "test 0.56711215\n",
      "87\n",
      "train 0.5652467\n",
      "test 0.56692123\n",
      "88\n",
      "train 0.5650073\n",
      "test 0.56649685\n",
      "89\n",
      "train 0.5653374\n",
      "test 0.56562704\n",
      "90\n",
      "train 0.564847\n",
      "test 0.5673667\n",
      "91\n",
      "train 0.5645826\n",
      "test 0.5667976\n",
      "92\n",
      "train 0.5661473\n",
      "test 0.5660052\n",
      "93\n",
      "train 0.5634903\n",
      "test 0.5653759\n",
      "94\n",
      "train 0.5640291\n",
      "test 0.56745064\n",
      "95\n",
      "train 0.56435275\n",
      "test 0.566656\n",
      "96\n",
      "train 0.5641889\n",
      "test 0.5664124\n",
      "97\n",
      "train 0.56443644\n",
      "test 0.56522596\n",
      "98\n",
      "train 0.5640946\n",
      "test 0.5652928\n",
      "99\n",
      "train 0.5642451\n",
      "test 0.5666522\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    print(epoch)\n",
    "    for x,y in gen_batches(train_dataset, batch_count=32):\n",
    "        loss = model.train_on_batch(x, y)\n",
    "        losses.append(loss)\n",
    "    train_loss = np.mean(losses)\n",
    "        \n",
    "    losses = []\n",
    "    for x,y in gen_batches(test_dataset, batch_count=32):\n",
    "        loss = model.test_on_batch(x, y)\n",
    "        losses.append(loss)\n",
    "    test_loss = np.mean(losses)\n",
    "    print(\"train \"+str(train_loss))\n",
    "    print(\"test \"+str(test_loss))\n",
    "    #write_log(callback, ['train', 'test'], [train_loss, test_loss], epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review how model works in production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\tO\tO\n",
      ".:\tO\tO\n",
      "et:\tO\tO\n",
      "locataires:\tO\tO\n",
      ",:\tO\tO\n",
      ",:\tO\tO\n",
      "dГ©gagГ©e:\tO\tO\n",
      "verrez:\tO\tO\n",
      "CORDONNIERS:\tO\tO\n",
      "Husselein:\tI-PER\tI-PER\n",
      "internat:\tO\tO\n",
      "89:\tO\tO\n",
      ",:\tO\tO\n",
      "niveau:\tO\tO\n",
      "AcadГ©mie:\tI-ORG\tI-ORG\n",
      "toujours:\tO\tO\n",
      ",:\tO\tO\n",
      "il:\tO\tO\n",
      "d':\tO\tO\n",
      "parmi:\tO\tO\n",
      "qui:\tO\tO\n",
      "mГ©decine:\tI-ORG\tO\n",
      ",:\tO\tO\n",
      "par:\tO\tO\n",
      "et:\tO\tO\n",
      "de:\tO\tO\n",
      "ВЈr:\tO\tO\n",
      ",:\tO\tO\n",
      "a:\tO\tO\n",
      "coiffeur:\tO\tO\n",
      "exister:\tO\tO\n",
      ",:\tO\tO\n",
      "entr'ouverte:\tO\tO\n",
      ".:\tO\tO\n",
      "utiles:\tO\tO\n",
      "dans:\tO\tO\n",
      "oГ№:\tO\tO\n",
      "sur:\tO\tO\n",
      "Seine:\tI-ORG\tI-LOC\n",
      "domicile:\tO\tO\n",
      "':\tO\tO\n",
      "вЂ“:\tO\tO\n",
      "Rennes:\tI-LOC\tI-LOC\n",
      "des:\tO\tO\n",
      "Г©lГЁvent:\tO\tO\n",
      "de:\tO\tO\n",
      "Grenelle:\tI-LOC\tI-LOC\n",
      "la:\tO\tO\n",
      ".:\tO\tO\n",
      ".:\tO\tO\n",
      "entiГЁre:\tO\tO\n",
      "ArrivГ©s:\tO\tO\n",
      "reprend:\tO\tO\n",
      "lots:\tO\tO\n",
      "inf:\tO\tO\n",
      "usines:\tO\tO\n",
      ",:\tO\tO\n",
      "73В»:\tO\tO\n",
      "DE:\tO\tO\n",
      "Le:\tO\tO\n",
      "C':\tO\tO\n",
      "prГ©sidГ©e:\tO\tO\n",
      "Emile-Zola:\tO\tI-LOC\n",
      "avancer:\tO\tO\n",
      "salles:\tO\tO\n",
      "1900:\tO\tO\n",
      ".:\tO\tO\n",
      "de:\tO\tO\n",
      "tiers:\tO\tO\n",
      "rencontre:\tO\tO\n",
      "dГ©:\tO\tO\n",
      "la:\tO\tO\n",
      "ces:\tO\tO\n",
      "ministГ©rielle:\tO\tO\n",
      "mourant:\tO\tO\n",
      "regards:\tO\tO\n",
      "de:\tO\tO\n",
      "sur:\tO\tO\n",
      "courant:\tO\tO\n",
      "l':\tO\tO\n",
      "que:\tO\tO\n",
      "de:\tO\tO\n",
      "porte:\tO\tO\n",
      "et:\tO\tO\n",
      "du:\tO\tO\n",
      "as:\tO\tO\n",
      "carte:\tO\tO\n",
      ".:\tO\tO\n",
      "journal:\tO\tO\n",
      "connaissance:\tO\tO\n",
      "fer:\tO\tO\n",
      "guerre:\tO\tO\n",
      "-il:\tO\tO\n",
      ".:\tO\tO\n",
      "CHARBONS:\tO\tO\n",
      ",:\tO\tO\n",
      "d':\tO\tO\n",
      ".:\tO\tO\n",
      "dГ©laissГ©es:\tO\tO\n",
      "fВ»:\tO\tO\n"
     ]
    }
   ],
   "source": [
    "query = test_dataset[160:260]\n",
    "query_words = [x[0] for x in query]\n",
    "query_types = [x[1] for x in query]\n",
    "result = model.predict_on_batch(encode_text(\" \".join(query_words)).reshape((1, -1)))[0]\n",
    "for index in range(result.shape[0]):\n",
    "    w = query_words[index]\n",
    "    true_type = query_types[index]\n",
    "    pred_type = unique_types[np.argmax(result[index,:])] \n",
    "    print(\"{}:\\t{}\\t{}\".format(w, pred_type, true_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home task\n",
    "\n",
    "- 3 points: make the model better\n",
    "- 7 points: implement the model with CRF layer (https://github.com/Hironsan/keras-crf-layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam**\n",
    "\n",
    "76\n",
    "train 0.067345284\n",
    "test 0.20488992\n",
    "\n",
    "77\n",
    "train 0.06869629\n",
    "test 0.19885963\n",
    "\n",
    "78\n",
    "train 0.066755325\n",
    "test 0.19162706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adamax**\n",
    "\n",
    "2\n",
    "train 0.068575904\n",
    "test 0.19806913\n",
    "\n",
    "3\n",
    "train 0.06759596\n",
    "test 0.18924537\n",
    "\n",
    "4\n",
    "train 0.06957598\n",
    "test 0.18730828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
